# Projeto de Engenharia de Dados | Pipeline ELT para a TechStyle Commerce

Este reposit√≥rio cont√©m o desenvolvimento de um pipeline de dados completo e automatizado, simulando um ambiente corporativo para a empresa fict√≠cia "TechStyle Commerce". O projeto foi criado como um case pr√°tico para demonstrar habilidades em engenharia de dados, desde a ingest√£o de fontes brutas at√© a disponibiliza√ß√£o de dashboards para an√°lise de neg√≥cio.

## üìå Vis√£o Geral do Projeto

A **TechStyle Commerce**, um e-commerce de eletr√¥nicos, enfrentava desafios com dados descentralizados e processos manuais de gera√ß√£o de relat√≥rios. Este projeto soluciona esse problema atrav√©s da constru√ß√£o de um pipeline de dados ELT (Extract, Load, Transform) que centraliza, processa e modela os dados de vendas, clientes e produtos em um Data Warehouse, servindo como uma fonte √∫nica da verdade (*Single Source of Truth*).

O resultado final √© um Data Mart confi√°vel que alimenta um dashboard de BI, permitindo que a equipe de an√°lise tome decis√µes estrat√©gicas baseadas em dados consistentes e atualizados.

## üéØ Objetivos do Projeto

* **Centralizar Dados:** Unificar dados de diferentes fontes (pedidos, pagamentos, clientes, produtos) em um √∫nico local.
* **Automatizar Processos:** Criar um fluxo de trabalho orquestrado que executa as etapas de ingest√£o e transforma√ß√£o de forma agendada e autom√°tica.
* **Modelagem Dimensional:** Estruturar os dados em um modelo dimensional (Star Schema) com tabelas Fato e Dimens√£o para otimizar consultas anal√≠ticas.
* **Garantir a Qualidade dos Dados:** Implementar testes para validar a integridade, consist√™ncia e regras de neg√≥cio dos dados.
* **Democratizar o Acesso aos Dados:** Disponibilizar os dados tratados para a √°rea de neg√≥cio atrav√©s de uma ferramenta de Business Intelligence.

## Diagrama da Arquitetura

A solu√ß√£o foi desenhada para ser modular, escal√°vel e baseada em tecnologias open-source amplamente utilizadas no mercado.

```
+----------------+      +----------------+      +-------------------+      +-----------------+      +---------------------+      +-----------------+      +------------------+
| Fontes de Dados|  ->  | Data Lake      |  ->  | Ingest√£o (Python) |  ->  | Data Warehouse  |  ->  | Transforma√ß√£o (dbt) |  ->  | Camada Anal√≠tica|  ->  | Ferramenta de BI |
| (Arquivos CSV) |      | (File System)  |      | (Orquestrado com  |      | (PostgreSQL)    |      | (SQL)               |      | (Marts)         |      | (Metabase)       |
|                |      |                |      |    Airflow)       |      |                 |      |                     |      |                 |      |                  |
+----------------+      +----------------+      +-------------------+      +-----------------+      +---------------------+      +-----------------+      +------------------+
```

## üìä Demonstra√ß√£o: Dashboard de An√°lise de Vendas

O dashboard final permite uma vis√£o clara dos principais KPIs (Key Performance Indicators) do neg√≥cio, como receita mensal, distribui√ß√£o de clientes e performance de produtos.


## üöÄ Como Executar o Projeto

### Pr√©-requisitos
* [Git](https://git-scm.com/)
* [Docker](https://www.docker.com/products/docker-desktop/)
* [Docker Compose](https://docs.docker.com/compose/)

### Passos para Instala√ß√£o

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/seu-usuario/nome-do-repositorio.git](https://github.com/seu-usuario/nome-do-repositorio.git)
    cd nome-do-repositorio
    ```

2.  **Configure as vari√°veis de ambiente:**
    Se houver um arquivo `.env.example`, renomeie-o para `.env`. Caso contr√°rio, crie um arquivo `.env` e configure as vari√°veis necess√°rias (como senhas para o PostgreSQL).

3.  **Suba os cont√™ineres Docker:**
    Este comando ir√° construir as imagens e iniciar todos os servi√ßos (Airflow, PostgreSQL, Metabase).
    ```bash
    docker-compose up -d --build
    ```

4.  **Acesse o Airflow:**
    * Abra seu navegador e acesse `http://localhost:8080`.
    * Use o login e senha padr√£o (`airflow`/`airflow`).
    * Ative a DAG `dag_pipeline_techstyle` no painel principal para iniciar o pipeline.

5.  **Acesse o Metabase e configure o Dashboard:**
    * Acesse `http://localhost:3000`.
    * Siga os passos de configura√ß√£o inicial.
    * Adicione uma nova conex√£o de banco de dados, apontando para o container do PostgreSQL. Use as credenciais definidas no `docker-compose.yml` ou no seu arquivo `.env`. O nome do host do banco de dados ser√° o nome do servi√ßo no Docker Compose (ex: `postgres_dw`).
    * Comece a criar suas perguntas e seu dashboard!

---

## üõ†Ô∏è Tecnologias, Arquitetura e Ferramentas Utilizadas

A seguir, uma lista das principais tecnologias e conceitos aplicados neste projeto:

### ‚öôÔ∏è Infraestrutura e Orquestra√ß√£o
* **Docker & Docker Compose:** Containeriza√ß√£o de todos os servi√ßos para garantir um ambiente de desenvolvimento e produ√ß√£o consistente e isolado.
* **Apache Airflow:** Ferramenta open-source para orquestrar, agendar e monitorar os workflows de dados (DAGs).
* **PostgreSQL:** Utilizado tanto como backend para o Airflow quanto como o Data Warehouse anal√≠tico para armazenar os dados tratados.

### üìä Processamento e Modelagem de Dados
* **Python:** Linguagem principal para os scripts de ingest√£o de dados, utilizando a biblioteca **Pandas** para manipula√ß√£o e leitura dos arquivos.
* **dbt (data build tool):** Ferramenta para a etapa de transforma√ß√£o (T) do ELT. Permite construir modelos de dados com SQL de forma modular, test√°vel e documentada.
* **SQL:** Linguagem utilizada para todas as transforma√ß√µes, agrega√ß√µes e modelagem de dados dentro do dbt.

### üìà Visualiza√ß√£o de Dados
* **Metabase:** Ferramenta de Business Intelligence open-source, de f√°cil configura√ß√£o, utilizada para criar o dashboard final e democratizar o acesso aos dados.

### üèõÔ∏è Arquitetura e Conceitos
* **ELT (Extract, Load, Transform):** Paradigma moderno de data integration onde os dados brutos s√£o primeiro carregados no Data Warehouse e transformados posteriormente.
* **Data Lake (simulado):** Utiliza√ß√£o do sistema de arquivos local para armazenar os dados brutos (raw data) antes do processamento.
* **Data Warehouse:** Banco de dados relacional otimizado para consultas anal√≠ticas (OLAP).
* **Modelagem Dimensional (Star Schema):** Metodologia para organizar os dados em tabelas Fato (eventos, m√©tricas) e Dimens√£o (contexto, atributos), facilitando a an√°lise.
* **Qualidade de Dados (Data Quality):** Implementa√ß√£o de testes automatizados com `dbt test` para garantir a confiabilidade dos dados no Data Mart.

### üîß Ferramentas e Boas Pr√°ticas
* **Git & GitHub:** Sistema de controle de vers√£o para gerenciamento do c√≥digo-fonte e documenta√ß√£o.
* **Visual Studio Code:** Editor de c√≥digo com extens√µes para Docker, Python e SQL.

---

## üó∫Ô∏è Roadmap (pr√≥ximos passos do andamento do projeto)
- [x] ‚úÖ Defini√ß√£o do Problema e Arquitetura
- [x] ‚úÖ Configura√ß√£o do Controle de Vers√£o
- [x] ‚úÖ Gera√ß√£o dos Dados de Origem (Fontes)
- [x] ‚úÖ "Dockerizar" o Projeto
- [x] ‚úÖ Estruturar o Projeto Airflow
- [x] ‚úÖ Desenvolver o Script de Ingest√£o
- [x] ‚úÖ Criar a DAG de Ingest√£o no Airflow 
- [ ] Configurar o Projeto dbt  
- [ ] Criar Modelos de Staging (staging) 
- [ ] Criar Modelos Dimensionais e de Fatos (marts)  
- [ ] Implementar Testes de Qualidade  
- [ ] Integrar dbt com Airflow 
- [ ] Configurar a Ferramenta de BI
